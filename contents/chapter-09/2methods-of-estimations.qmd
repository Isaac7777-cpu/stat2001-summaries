---
title: "Methods of Estimations"
---

## Learning Goals

> Now, it is not always obvious as to how the point estimator is derived...
> 
>  For example, using twice the average of the sample to estimate the upper bound of a continuous uniform distribution seem to be out of thin air. And, multiple other estimators. 

Therefore, we will look at two general methods for deriving point estimators:

1. the _method of moments_ (MOME)
2. the _method of maximum likelihood_ (MLE)

## Method of Moments

Consider a random sample $Y_1, Y_2, \ldots , Y_n$ from some distribution. Thus these random variables are $\iid$ Recall that $Y \triangleq Y_1$ has **$k$th raw moment** $\mu_k' = \E{Y^k}$. We now also, analogous to sample mean, define the **$k$th sample moment** as $m_k' = \frac{1}{n} \sum_{i=1}^n y_i^k$.

Suppose that the distribution of $Y$ involves $t$ unknown parameters $t = 1, 2, 3, \ldots$. Then the **method of moments (MOM)** involves solving the following system of equations.
\begin{equation}
\begin{aligned}
\mu_1' &= m_1' \\
\mu_2' &= m_2' \\
\vdots &= \vdots \\
\mu_t' &= m_t' \\
\end{aligned}
\end{equation}

The solution of these $t$ equations leads to **the method of moments estimates (MOME's)** of the $t$ unknown parameters.

### Some Intuitions

> This seems to be a relatively unintuitive thing to do. Or why do we want to match the moment? There are many other things we can match to as well...

The idea here is that the $k$th sample moment as a random variable, $M_k' = \frac{1}{n}\sum_{i=1}^n Y_i^k$, has mean $\mu_k'$ and a variance that converges to zero as $n$ to infinity:
\begin{gather*}
\E{M_k'} = \frac{1}{n} \sum_{i=1}^n \E{Y_i^k} = \frac{1}{n} \sum_{i=1}^n \mu_k' = \mu_k' \\
\Var{M_k'} = \frac{1}{n^2} \sum_{i = 1}^n \Var{Y_i^k} = \frac{n \Var{Y_1^k}}{n^2} = \frac{\Var{Y_1^k}}{n} \to 0 \\
\end{gather*}

Thus, by [Theorem 1](1properties.html#formalisation), (or equivalently by law of large number), $M_k'$ is a consistent estimator of $\mu_k'$. That is, for each $k$ and any $\varepsilon > 0$, $\P{\abs{M_k' - \mu_k'
} > \varepsilon} \to 0$ as $n \to \infty$. So, if $n$ is large, each $m_k'$ shoud be close to $\mu_k'$.

In some cases the above definition of the MOM may require slight modification because the system of equations as indicated cannot be solved for the $t$ unknown parameters, For example, if $\mu_1' = 0$, it may be necessary to also equate $\mu_{t+1}' = m_{t+1}'$.

---

#### Example

::: {#exm-4}

Consider a random sample of numbers from between $0$ and $c$. Find the method of moments estimate of $c$.

:::{.separator}
:::

::: {.solution}

Here: $t = 1$, $\mu_1' = \E{Y} = c / 2, m_1' = \mean{y}$. We now equate $\mu_1' = m_1'$. This implies that $c/2 = \mean{y}$ and hence $c = 2 \mean{y}$. Thus the method of moments estimate of $c$ is $\hat{c}  = 2 \mean{y}$. In other words, the method of moments estimator of $c$ is $\hat{c} = 2\mean{Y}$. Note that this estimator is both unbiased and consistent. 

:::

:::

::: {.callout-note title="Estimate vs Estimator" collapse="false"}

Note that the estimates is a constant while the estimator is a random variable.

... BUT, it is common for these two terms to be used interchangeable. Thus, when we say that an estimate is unbiased or consistent, it should be understood that we are saying this about the corresponding estimator.

:::

---

::: {#exm-5}

Suppose that $Y_1, Y_2, \ldots , Y_n \iidsim \GammaDist(a, b)$. Find the method of moments estimates of $a$ and $b$. 

:::{.separator}
:::

:::{.solution}

Here: $t = 2$, 

- $\mu_1' = \E{Y} = ab$, 
- $\mu_2' = \E{Y^2} = \Var{Y} = (\E{Y})^2 = ab^2 + (ab)^2$

Therefore, equating with $\mu_1' = m_1'$ and $\mu_2' = m_2'$. This implies that $ab = \mean{y}$ and $ab^2 + a^2b^2 = m_2'$. Solving these equations leads to the MOME's:
\begin{equation}
\hat{a} = \frac{\mean{y}^2}{m_2' - \mean{y}^2} \qquad , \qquad \hat{b} = \frac{\mean{y}}{\hat{a}}
\end{equation}

For example, suppose that the data values are $1.3$ and $2.7$. Then $\mean{y} = 2$ and $m_2' = 4.49$. Therefore $\hat{a} = 8.1633$ and $\hat{b} = 0.245$.

:::

:::

---

:::{#exm-mome-normal}

$Y_2, Y_2, \ldots , Y_n \iidsim \NormalDist(a, b^2)$. Find the MOME's of $a$ and $b^2$.

:::{.separator}
:::

:::{.solution}

$\mu_1' = a$, $\mu_2' = \E{Y^2} = a^2 + b^2$, $m_1' = \mean{y}$, $m_2' = (1/n) \sum_{i=1}^n y_i^2$. Equating with $\mu_t' = m_t'$, we get
\begin{equation}
\hat{a} = \mean{y} \qquad , \qquad \hat{b}^2 = \frac{n-1}{n}S^2 = \frac{1}{n}\sum_{i=1}^n (y_i - \mean{y})^2
\end{equation}
i.e.
\begin{align*}
a^2 + b^2 & = \frac{1}{n} \sum_{i=1}^n y_i^2 \\
b^2 & = \left(\frac{1}{n} \sum_{i = 1}^n y_i^2 \right) - \mean{y}^2 \\
& = \frac{1}{n} \left( \sum_{i=1}^n {y_i^2} - n\mean{y}^2 \right) = \frac{(n-1)}{n} S^2 \\
\end{align*}

Note that therefore $\hat{b}^2$ is biased in small samples but it is consistent.

:::

:::